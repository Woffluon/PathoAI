{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAvA_plxEVuT"
      },
      "source": [
        "## Hücre 1: Ortam Hazırlığı"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tga-DVrMEVuU"
      },
      "outputs": [],
      "source": [
        "import os, random, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "try:\n",
        "    policy = mixed_precision.Policy('mixed_bfloat16')\n",
        "    mixed_precision.set_global_policy(policy)\n",
        "    print(\"Policy: mixed_bfloat16 (Ampere/Hopper architecture optimized)\")\n",
        "except:\n",
        "    policy = mixed_precision.Policy('mixed_float16')\n",
        "    mixed_precision.set_global_policy(policy)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(f\"GPU Availability: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkbPv3RlEVuV"
      },
      "source": [
        "## Hücre 2: Kaggle Dataset İndirme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P9tA0YnEVuV"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded:\n",
        "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "    os.replace(fn, f'/root/.kaggle/{fn}')\n",
        "    os.chmod(f'/root/.kaggle/{fn}', 0o600)\n",
        "\n",
        "os.makedirs('/content/datasets', exist_ok=True)\n",
        "\n",
        "print(\"[INFO] Dataset indiriliyor...\")\n",
        "!kaggle datasets download -d andrewmvd/lung-and-colon-cancer-histopathological-images \\\n",
        "    -p /content/datasets --unzip -q\n",
        "\n",
        "print(\"[OK] Dataset hazır: /content/datasets/lung_colon_image_set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF0sLACSEVuV"
      },
      "source": [
        "## Hücre 3: Veri Hazırlama ve Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYXWdOtKEVuW"
      },
      "outputs": [],
      "source": [
        "!pip install imagehash\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "LUNG_ROOT = Path('/content/datasets/lung_colon_image_set/lung_image_sets')\n",
        "mapping = {'lung_n': 0, 'lung_aca': 1, 'lung_scc': 2}\n",
        "\n",
        "print(\"[ANALİZ] Görüntü Hash'leri hesaplanıyor (Flip + Rotation Duyarlı)...\")\n",
        "\n",
        "def calculate_phash(image_path):\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        hashes = []\n",
        "\n",
        "        for image_version in [img, img.transpose(Image.FLIP_LEFT_RIGHT)]:\n",
        "            for angle in [0, 90, 180, 270]:\n",
        "                rot_img = image_version.rotate(angle, expand=True)\n",
        "                hashes.append(str(imagehash.phash(rot_img)))\n",
        "\n",
        "        return min(hashes)\n",
        "    except Exception as e:\n",
        "        print(f\"Hata: {e}\")\n",
        "        return None\n",
        "\n",
        "rows = []\n",
        "for cls_folder, label in mapping.items():\n",
        "    folder = LUNG_ROOT / cls_folder\n",
        "    files = list(folder.rglob('*.jpeg')) + list(folder.rglob('*.jpg'))\n",
        "\n",
        "    print(f\"   Klasör taranıyor: {cls_folder} ({len(files)} dosya)\")\n",
        "    for img_path in tqdm(files, desc=f\"{cls_folder}\", leave=False):\n",
        "        h = calculate_phash(img_path)\n",
        "        if h:\n",
        "            rows.append({'filename': str(img_path), 'label': label, 'img_hash': h})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "print(\"[İŞLEM] Benzer görüntüler gruplanıyor...\")\n",
        "df['group_id'] = df.groupby('img_hash').ngroup()\n",
        "\n",
        "unique_groups = df['group_id'].nunique()\n",
        "print(f\"\\n[SONUÇ] Toplam Görüntü: {len(df)}\")\n",
        "print(f\"[SONUÇ] Tespit Edilen Benzersiz Grup Sayısı: {unique_groups}\")\n",
        "print(f\"        (Beklenen: ~750 - 2000 arası olmalı)\")\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
        "train_idx, test_idx = next(gss.split(df, groups=df['group_id']))\n",
        "\n",
        "train_df = df.iloc[train_idx]\n",
        "temp_df = df.iloc[test_idx]\n",
        "\n",
        "gss_val = GroupShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
        "val_idx, test_final_idx = next(gss_val.split(temp_df, groups=temp_df['group_id']))\n",
        "\n",
        "val_df = temp_df.iloc[val_idx]\n",
        "test_df = temp_df.iloc[test_final_idx]\n",
        "\n",
        "# Kaydetme işlemleri...\n",
        "os.makedirs('/content/data_csv_leakage_proof', exist_ok=True)\n",
        "train_df.to_csv('/content/data_csv_leakage_proof/train.csv', index=False)\n",
        "val_df.to_csv('/content/data_csv_leakage_proof/val.csv', index=False)\n",
        "test_df.to_csv('/content/data_csv_leakage_proof/test.csv', index=False)\n",
        "\n",
        "print(\"\\n[BİLGİ] Yeni dağılım:\")\n",
        "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
        "print(\"[ONAY] Train setindeki hiçbir görüntünün kopyası (döndürülmüş veya ters çevrilmiş hali) Test setinde YOK.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhMa5lCJEVuW"
      },
      "source": [
        "## Hücre 4: tf.data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5udCad7EVuX"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def create_dataset_paper_impl(df, batch_size, is_training=False):\n",
        "    \"\"\"\n",
        "    Tummala et al. (2023) Makale Stratejisi + 56GB RAM Optimizasyonu\n",
        "    \"\"\"\n",
        "\n",
        "    def load_and_crop(path, label):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "\n",
        "        img = tf.image.central_crop(img, central_fraction=0.75)\n",
        "        img = tf.image.resize(img, IMG_SIZE)\n",
        "\n",
        "        img = tf.cast(img, tf.float32)\n",
        "        return img, label\n",
        "\n",
        "    def augment(img, label):\n",
        "\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_flip_up_down(img)\n",
        "\n",
        "\n",
        "        k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
        "        img = tf.image.rot90(img, k)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    paths = df['filename'].values\n",
        "    labels = df['label'].astype(float).values\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    ds = ds.map(load_and_crop, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    ds = ds.cache()\n",
        "\n",
        "    if is_training:\n",
        "        ds = ds.shuffle(10000, seed=SEED)\n",
        "        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "print(\"[INFO] Pipeline oluşturuluyor (RAM Cache devrede)...\")\n",
        "train_ds = create_dataset_paper_impl(train_df, BATCH_SIZE, is_training=True)\n",
        "val_ds = create_dataset_paper_impl(val_df, BATCH_SIZE, is_training=False)\n",
        "test_ds = create_dataset_paper_impl(test_df, BATCH_SIZE, is_training=False)\n",
        "\n",
        "print(f\"[OK] Pipeline hazır. Batch Size: {BATCH_SIZE} (Makale uyumlu)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6PnkQscEVuX"
      },
      "source": [
        "## Hücre 5: Model Oluşturma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O46p9TZ-EVuX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import EfficientNetV2S\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "print(\"[INFO] Model: EfficientNetV2-S (Small) hazırlanıyor...\")\n",
        "\n",
        "base_model = EfficientNetV2S(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_preprocessing=True\n",
        ")\n",
        "\n",
        "total_layers = len(base_model.layers)\n",
        "split_point = int(total_layers * 0.5)\n",
        "\n",
        "print(f\"[INFO] Toplam Parametre: ~21M (Large modelin 1/6'sı)\")\n",
        "print(f\"[INFO] Toplam Katman: {total_layers}. Fine-tuning: {split_point}. katmandan itibaren.\")\n",
        "\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:split_point]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in base_model.layers[split_point:]:\n",
        "    if isinstance(layer, layers.BatchNormalization):\n",
        "        layer.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape=(224, 224, 3))\n",
        "x = base_model(inputs)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(3, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "model = models.Model(inputs, outputs, name='EffNetV2S_Ablation_Study')\n",
        "\n",
        "optimizer = optimizers.Adadelta(learning_rate=0.1)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AvQkqmJEVuY"
      },
      "source": [
        "## Hücre 6: Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK961OiREVuY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
        "    CSVLogger, LearningRateScheduler\n",
        ")\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    \"\"\"Warmup for first 3 epochs\"\"\"\n",
        "    if epoch < 3:\n",
        "        return scaled_lr * (epoch + 1) / 3\n",
        "    return scaled_lr\n",
        "\n",
        "callbacks_phase1 = [\n",
        "    LearningRateScheduler(lr_schedule, verbose=0),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=8,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        '/content/resnet50_best.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=4,\n",
        "        min_lr=1e-7,\n",
        "        verbose=0\n",
        "    ),\n",
        "    CSVLogger('/content/training_log.csv')\n",
        "]\n",
        "\n",
        "callbacks_phase2 = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=8,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        '/content/resnet50_best.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=4,\n",
        "        min_lr=1e-7,\n",
        "        verbose=0\n",
        "    ),\n",
        "    CSVLogger('/content/training_log.csv', append=True)\n",
        "]\n",
        "\n",
        "print(\"[OK] Callbacks hazır\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAYb7e6DEVuY"
      },
      "source": [
        "## Hücre 7: Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea1CWknVEVuY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        '/content/effnetv2s_best.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    CSVLogger('/content/training_log_paper.csv'),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=4,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "class_weights = {\n",
        "    0: 1.0,\n",
        "    1: 1.1,\n",
        "    2: 1.1\n",
        "}\n",
        "\n",
        "print(\"[TRAINING] Başlıyor...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=40, # EarlyStopping durduracak\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_86XEg8EVuZ"
      },
      "source": [
        "## Hücre 8: Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9XHbPXtEVuZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    precision_score, recall_score, roc_curve, auc\n",
        ")\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class_names = ['Normal', 'Adenocarcinoma', 'Squamous']\n",
        "\n",
        "print(\"[INFO] Model yükleniyor...\")\n",
        "model = tf.keras.models.load_model('/content/effnetv2s_best.keras')\n",
        "\n",
        "print(\"[INFO] Test seti üzerinde ölçüm yapılıyor...\")\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "\n",
        "print(\"[INFO] Tahminler üretiliyor...\")\n",
        "y_pred_probs = model.predict(test_ds, verbose=0)\n",
        "y_true = test_df['label'].values\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEST SONUÇLARI - 3 SINIF\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "print(f\"Loss: {test_loss:.4f}\")\n",
        "print(f\"\\nSınıflar: {class_names}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(f\"\\nSınıf Başına Doğruluk:\")\n",
        "for i, name in enumerate(class_names):\n",
        "    correct = cm[i, i]\n",
        "    total = cm[i, :].sum()\n",
        "    if total > 0:\n",
        "        print(f\"  {name}: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"  {name}: 0/0 (Veri yok)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FR7E068EVuZ"
      },
      "source": [
        "## Hücre 9: Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPwXTY0OEVuZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"[INFO] En iyi model yükleniyor...\")\n",
        "model_path = '/content/effnetv2s_best.keras'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "print(\"[INFO] Test seti üzerinde tahmin yapılıyor...\")\n",
        "y_pred_probs = model.predict(test_ds, verbose=1)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "y_true = test_df['label'].values.astype(int)\n",
        "\n",
        "class_names = ['Normal', 'Adenocarcinoma', 'Squamous']\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST SONUÇLARI (EfficientNetV2-S)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_acc = np.sum(y_pred == y_true) / len(y_true)\n",
        "print(f\"Genel Doğruluk (Accuracy): {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nSınıflandırma Raporu:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SINIF BAZLI DETAYLI ANALİZ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, name in enumerate(class_names):\n",
        "    mask = (y_true == i)\n",
        "    correct = (y_pred[mask] == i).sum()\n",
        "    total = mask.sum()\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, labels=[i], average=None, zero_division=0)[0]\n",
        "    recall = recall_score(y_true, y_pred, labels=[i], average=None, zero_division=0)[0]\n",
        "    f1 = f1_score(y_true, y_pred, labels=[i], average=None, zero_division=0)[0]\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Doğru/Toplam: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.xlabel('Tahmin Edilen (Predicted)', fontsize=12)\n",
        "plt.ylabel('Gerçek (True)', fontsize=12)\n",
        "plt.title('Confusion Matrix (EfficientNetV2-S)', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.xlabel('Tahmin Edilen', fontsize=12)\n",
        "plt.ylabel('Gerçek', fontsize=12)\n",
        "plt.title('Normalized Confusion Matrix', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 10: Grafik"
      ],
      "metadata": {
        "id": "6CIonu8Th-Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def plot_training_history_hd(history=None, log_path='/content/training_log_paper.csv'):\n",
        "    \"\"\"\n",
        "    Eğitim grafiklerini (Accuracy & Loss) yayın kalitesinde (Publication Quality) çizer.\n",
        "    Bellekteki 'history' objesini veya CSV log dosyasını kullanabilir.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        if history is not None:\n",
        "            acc = history.history['accuracy']\n",
        "            val_acc = history.history['val_accuracy']\n",
        "            loss = history.history['loss']\n",
        "            val_loss = history.history['val_loss']\n",
        "            source = \"Memory\"\n",
        "        elif os.path.exists(log_path):\n",
        "            df = pd.read_csv(log_path)\n",
        "            acc = df['accuracy'].values\n",
        "            val_acc = df['val_accuracy'].values\n",
        "            loss = df['loss'].values\n",
        "            val_loss = df['val_loss'].values\n",
        "            source = \"CSV Log\"\n",
        "        else:\n",
        "            print(\"HATA: Veri kaynağı bulunamadı.\")\n",
        "            return\n",
        "    except Exception as e:\n",
        "        print(f\"Veri okuma hatası: {e}\")\n",
        "        return\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.rcParams.update({\n",
        "        'font.family': 'sans-serif',\n",
        "        'font.sans-serif': ['Arial', 'DejaVu Sans'],\n",
        "        'font.size': 12,\n",
        "        'axes.titlesize': 16,\n",
        "        'axes.labelsize': 14,\n",
        "        'xtick.labelsize': 12,\n",
        "        'ytick.labelsize': 12,\n",
        "        'legend.fontsize': 12,\n",
        "        'figure.dpi': 300,\n",
        "        'axes.linewidth': 1.5,\n",
        "        'grid.alpha': 0.3\n",
        "    })\n",
        "\n",
        "    color_train = '#0056b3'\n",
        "    color_val = '#d62728'\n",
        "    color_best = '#2ca02c'\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7), constrained_layout=True)\n",
        "    fig.suptitle(f'Model Training Performance (Source: {source})', fontsize=20, weight='bold', y=1.05)\n",
        "\n",
        "    ax1.plot(epochs, acc, label='Training Accuracy', color=color_train, linewidth=2.5, alpha=0.9)\n",
        "    ax1.plot(epochs, val_acc, label='Validation Accuracy', color=color_val, linewidth=2.5, alpha=0.9)\n",
        "\n",
        "    ax1.fill_between(epochs, acc, val_acc, color='gray', alpha=0.1)\n",
        "\n",
        "    best_acc_epoch = np.argmax(val_acc) + 1\n",
        "    best_acc_val = np.max(val_acc)\n",
        "\n",
        "    ax1.axvline(x=best_acc_epoch, color=color_best, linestyle='--', alpha=0.7, linewidth=1.5)\n",
        "    ax1.scatter(best_acc_epoch, best_acc_val, s=150, c=color_best, edgecolors='white', zorder=5)\n",
        "\n",
        "    ax1.annotate(\n",
        "        f'Best Accuracy\\n{best_acc_val:.4f}\\n(Epoch {best_acc_epoch})',\n",
        "        xy=(best_acc_epoch, best_acc_val),\n",
        "        xytext=(10, -40), textcoords='offset points',\n",
        "        bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=color_best, alpha=0.9),\n",
        "        arrowprops=dict(arrowstyle=\"->\", color=color_best, connectionstyle=\"arc3,rad=.2\"),\n",
        "        ha='left', fontsize=11, color='#333333'\n",
        "    )\n",
        "\n",
        "    ax1.set_title('Accuracy Evolution', weight='bold', pad=15)\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(loc='lower right', frameon=True, framealpha=0.9, shadow=True)\n",
        "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
        "    ax1.spines['top'].set_visible(False)\n",
        "    ax1.spines['right'].set_visible(False)\n",
        "\n",
        "    ax2.plot(epochs, loss, label='Training Loss', color=color_train, linewidth=2.5, alpha=0.9)\n",
        "    ax2.plot(epochs, val_loss, label='Validation Loss', color=color_val, linewidth=2.5, alpha=0.9)\n",
        "\n",
        "    ax2.fill_between(epochs, loss, val_loss, color='gray', alpha=0.1)\n",
        "\n",
        "    best_loss_epoch = np.argmin(val_loss) + 1\n",
        "    best_loss_val = np.min(val_loss)\n",
        "\n",
        "    ax2.axvline(x=best_loss_epoch, color=color_best, linestyle='--', alpha=0.7, linewidth=1.5)\n",
        "    ax2.scatter(best_loss_epoch, best_loss_val, s=150, c=color_best, edgecolors='white', zorder=5)\n",
        "\n",
        "    ax2.annotate(\n",
        "        f'Best Loss\\n{best_loss_val:.5f}\\n(Epoch {best_loss_epoch})',\n",
        "        xy=(best_loss_epoch, best_loss_val),\n",
        "        xytext=(10, 40), textcoords='offset points',\n",
        "        bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=color_best, alpha=0.9),\n",
        "        arrowprops=dict(arrowstyle=\"->\", color=color_best, connectionstyle=\"arc3,rad=.2\"),\n",
        "        ha='left', fontsize=11, color='#333333'\n",
        "    )\n",
        "\n",
        "    ax2.set_title('Loss Convergence', weight='bold', pad=15)\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(loc='upper right', frameon=True, framealpha=0.9, shadow=True)\n",
        "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
        "    ax2.spines['top'].set_visible(False)\n",
        "    ax2.spines['right'].set_visible(False)\n",
        "\n",
        "    save_path = '/content/training_results_hd'\n",
        "    plt.savefig(f'{save_path}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(f'{save_path}.pdf', bbox_inches='tight')\n",
        "    plt.savefig(f'{save_path}.svg', bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "    print(f\"[INFO] Grafikler kaydedildi:\")\n",
        "    print(f\"   - PNG (300 DPI): {save_path}.png\")\n",
        "    print(f\"   - PDF (Vector):  {save_path}.pdf (Rapor için bunu kullanın)\")\n",
        "\n",
        "# Çalıştırma\n",
        "try:\n",
        "    plot_training_history_hd(history)\n",
        "except NameError:\n",
        "    plot_training_history_hd()"
      ],
      "metadata": {
        "id": "616No6VXiBAD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}